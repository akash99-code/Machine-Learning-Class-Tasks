{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reveiw Machine Learning: SVM, Decision Trees and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is a support vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. Support vectors are the data points that lie closest to the decision surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Do you scale inputs when using SVMs? why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: SVM try to fit the largest possible margin between the classes, so if the training set is not scaled, the SVM will tend to neglect small features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundred of fetures?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. The Primal form of the SVM.\n",
    "The computational complexity of the primal form of the SVM problem is proportional to the number of training instances m, while the computational complexity of the dual form is proportional to a number between m¬≤ and m¬≥.\n",
    "The dual problem is faster to solve than the primal when the number of training instances is smaller than the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Say you've trained an SVM classifier with an RBF kernel, but it seems to underfit the training set. Should you increase or decrese $\\gamma$? What about C?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. We shoud increase ùõæ or C or both, in order to decrease regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What is the fundamental idea behind SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. The fundamental idea behind Support Vector Machines is to fit the widest possible margin between the decision boundary that separates the two classes of\n",
    "the training instances. When performing soft margin classification, the SVM\n",
    "searches for a compromise between perfectly separating the two classes and having the widest possible margin.\n",
    "Another key idea is to use kernels when training on nonlinear datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What is the approximate depth of a Decision Tree trained (without restrictions) on a training set with one million instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. O(log2(m)). A well balanced binary decision tree will have log2(m)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Is a node's Gini impurity generally lower or greater than it's parent's node? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. Generally lower. CART training algorithm's cost function ensures that  splitting of each node is done in a way that minimizes the weighted sum of its children's Gini impurities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. If a Decision Tree is overfitting the training set, is it a good idea to try decreasing `max_depth`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. Yes. It will increase regularization effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. No effect of Scaling training set for Decision tree algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. If you have trained five different models on the exact same training data, and they all achieve $95\\%$ precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. We can use Voting Ensemble for combining the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. What is the difference between hard and soft voting classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. Hard Voting Classifier picks the class which was voted by majority of the models.\n",
    "Soft Voting Classifier applies a weighted average on the outomes of the models to give final prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. What is the benefit of out-of-bag evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. Due to bootstrapping in Bagging ensemble, some instances are never selected as input the learners. This instances can be used as test intances to perform the evaluation and thus there is no need to reserve validation set at starting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. If your AdaBoost ensemble underfits the training data, which hyperparameters should you tweak and how? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. We can :\n",
    "- increase no. of estimators\n",
    "- reduce regularization hyperparameters of the base estimator\n",
    "- increase the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. Increase the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. What are advantages and disadvantages of Decision Trees? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n",
    "Advantages:\n",
    "- No Transformations on input set is required\n",
    "- High Interpretability\n",
    "- Can Capture non-linear relationships\n",
    "- Decision tree is non-parametric\n",
    "\n",
    "Disadvantages:\n",
    "- Easily Overfits\n",
    "- Training Time Complexity is very high, and thus can't use in big data. \n",
    "- It is an unstable model, i.e, model changes drastically with little change in input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16.  Explain the terms: Gini Index, Classification Error, Entropy for two-classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.   \n",
    "Gini Index\n",
    "- It is used calculate Entropy of a node.\n",
    "- $ gini = 1- \\sum_{i=1}^{n}{P_i^2} $    \n",
    "\n",
    "Classification Error\n",
    "- no. of mis-classification/ no. of classification\n",
    "\n",
    "Entropy\n",
    "- It is the measurement of the impurity or randomness in the data points.\n",
    "- Entropy is the lowest for no disorder and maximum for high disorder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. What are the potential benefits of decision tree pruning?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.  \n",
    "- Pruning reduces the complexity of the final classifier\n",
    "- Improves predictive accuracy by the reduction of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Explain the trade-off between precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. \n",
    "We can‚Äôt have both precision and recall high. If we increase precision, it will reduce recall, and vice versa. This is called the precision/recall tradeoff.\n",
    "It is because if we try to decrease false positive,by increasing some threshold, it increases Precision but also increases False Negatives and thus Recall decreases, and vice versa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
